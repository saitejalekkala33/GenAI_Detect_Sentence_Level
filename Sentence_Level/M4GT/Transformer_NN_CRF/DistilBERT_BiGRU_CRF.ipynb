{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ebf8b04-bb2b-455a-b218-8947287e430c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 16:27:58.734492: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n",
      "Train Loss: 80.6846\n",
      "Validation Metrics:\n",
      "Accuracy: 0.9317\n",
      "Precision: 0.9330\n",
      "Recall: 0.9317\n",
      "F1 Score: 0.9320\n",
      "MCC: 0.8577\n",
      "MAE: 0.07±0.25\n",
      "Kappa: 0.8570\n",
      "New best model with Kappa Score: 0.8570\n",
      "\n",
      "Epoch 2/3\n",
      "Train Loss: 34.6524\n",
      "Validation Metrics:\n",
      "Accuracy: 0.9080\n",
      "Precision: 0.9099\n",
      "Recall: 0.9080\n",
      "F1 Score: 0.9066\n",
      "MCC: 0.8051\n",
      "MAE: 0.09±0.29\n",
      "Kappa: 0.8006\n",
      "\n",
      "Epoch 3/3\n",
      "Train Loss: 26.0526\n",
      "Validation Metrics:\n",
      "Accuracy: 0.9075\n",
      "Precision: 0.9094\n",
      "Recall: 0.9075\n",
      "F1 Score: 0.9061\n",
      "MCC: 0.8040\n",
      "MAE: 0.09±0.29\n",
      "Kappa: 0.7996\n",
      "\n",
      "Best model at epoch 1 with Kappa Score: 0.8570\n",
      "\n",
      "Test Results:\n",
      "Accuracy: 0.8778\n",
      "Precision: 0.8830\n",
      "Recall: 0.8778\n",
      "F1 Score: 0.8736\n",
      "MCC: 0.7321\n",
      "MAE: 0.12±0.33\n",
      "Kappa: 0.7194\n",
      "Boundary MAE ± SD: 30.9973 ± 60.0497\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchcrf import CRF\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, cohen_kappa_score\n",
    "\n",
    "class MixedTextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        boundary_idx = int(self.labels[idx])\n",
    "        words = text.split()\n",
    "        if len(words) > self.max_len - 2:\n",
    "            if boundary_idx > self.max_len - 2:\n",
    "                words = words[-(self.max_len - 2):]\n",
    "                boundary_idx = 0\n",
    "            else:\n",
    "                words = words[:self.max_len - 2]\n",
    "        word_labels = [0 if i <= boundary_idx else 1 for i in range(len(words))]\n",
    "        truncated_text = \" \".join(words)\n",
    "        encoding = self.tokenizer(\n",
    "            truncated_text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_special_tokens_mask=True\n",
    "        )\n",
    "        special_tokens_mask = encoding['special_tokens_mask'][0]\n",
    "        token_labels = []\n",
    "        current_word_idx = 0\n",
    "        for is_special in special_tokens_mask:\n",
    "            if is_special:\n",
    "                token_labels.append(-100)\n",
    "            else:\n",
    "                if current_word_idx < len(word_labels):\n",
    "                    token_labels.append(word_labels[current_word_idx])\n",
    "                    current_word_idx += 1\n",
    "                else:\n",
    "                    token_labels.append(-100)\n",
    "        token_labels = token_labels[:self.max_len]\n",
    "        if len(token_labels) < self.max_len:\n",
    "            token_labels.extend([-100] * (self.max_len - len(token_labels)))\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(token_labels)\n",
    "        }\n",
    "\n",
    "class DistilBERTBiGRUCRFTagger(nn.Module):\n",
    "    def __init__(self, model_name, num_labels, hidden_dim=512, num_layers=2, dropout=0.3):\n",
    "        super(DistilBERTBiGRUCRFTagger, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.distilbert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        hidden_size = self.distilbert.config.hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim * 2)\n",
    "        self.hidden2hidden = nn.Sequential(nn.Linear(hidden_dim * 2, hidden_dim * 2), nn.ReLU(), nn.Dropout(dropout))\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "        nn.init.xavier_uniform_(self.classifier.weight)\n",
    "        nn.init.constant_(self.classifier.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.distilbert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = self.dropout(outputs.last_hidden_state)\n",
    "        gru_out, _ = self.gru(sequence_output)\n",
    "        gru_out = self.layer_norm(gru_out)\n",
    "        gru_out = self.hidden2hidden(gru_out)\n",
    "        logits = self.classifier(gru_out)\n",
    "        if labels is not None:\n",
    "            mask = attention_mask.bool()\n",
    "            crf_labels = labels.clone()\n",
    "            crf_labels[crf_labels == -100] = 0\n",
    "            loss = -self.crf(logits, crf_labels, mask=mask, reduction='mean')\n",
    "            return loss\n",
    "        else:\n",
    "            mask = attention_mask.bool()\n",
    "            predictions = self.crf.decode(logits, mask=mask)\n",
    "            padded_predictions = []\n",
    "            for pred, mask_len in zip(predictions, attention_mask.sum(1).tolist()):\n",
    "                pad_len = attention_mask.size(1) - len(pred)\n",
    "                padded_pred = pred + [0] * pad_len\n",
    "                padded_predictions.append(padded_pred)\n",
    "            return torch.tensor(padded_predictions, device=input_ids.device)\n",
    "\n",
    "def train_model(model, data_loader, optimizer, scheduler, device, clip_value=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_ids, attention_mask, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_value)\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            predictions = model(input_ids, attention_mask)\n",
    "            for pred_seq, label_seq, mask_seq in zip(predictions, labels, attention_mask):\n",
    "                valid_indices = (mask_seq == 1) & (label_seq != -100)\n",
    "                valid_pred = pred_seq[valid_indices].cpu().numpy()\n",
    "                valid_label = label_seq[valid_indices].cpu().numpy()\n",
    "                all_predictions.extend(valid_pred)\n",
    "                all_labels.extend(valid_label)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_labels = np.array(all_labels)\n",
    "    absolute_errors = np.abs(all_predictions - all_labels)\n",
    "    mae = np.mean(absolute_errors)\n",
    "    std_dev = np.std(absolute_errors)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    mcc = matthews_corrcoef(all_labels, all_predictions)\n",
    "    kappa = cohen_kappa_score(all_labels, all_predictions)\n",
    "    return accuracy, precision, recall, f1, mcc, mae, std_dev, kappa\n",
    "\n",
    "def predict_boundary(model, text, tokenizer, max_len, device):\n",
    "    model.eval()\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\",\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "    input_ids = encoding[\"input_ids\"].to(device)\n",
    "    attention_mask = encoding[\"attention_mask\"].to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_ids, attention_mask)\n",
    "    pred_labels = predictions[0]\n",
    "    boundary_index = next((i for i, label in enumerate(pred_labels) if label == 1), len(pred_labels))\n",
    "    special_tokens_mask = encoding['special_tokens_mask'][0].tolist()\n",
    "    actual_tokens = [i for i, is_special in enumerate(special_tokens_mask) if not is_special]\n",
    "    if boundary_index < len(actual_tokens):\n",
    "        boundary_index = actual_tokens[boundary_index]\n",
    "    return boundary_index\n",
    "\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "config = {\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 2e-5,\n",
    "    'epochs': 3,\n",
    "    'max_len': 512,\n",
    "    'weight_decay': 0.01,\n",
    "    'gradient_clip': 1.0\n",
    "}\n",
    "\n",
    "train_df = pd.read_csv('sentence_train_data.csv')\n",
    "dev_df = pd.read_csv('sentence_dev_data.csv')\n",
    "test_df = pd.read_csv('sentence_test.csv')\n",
    "\n",
    "train_texts = train_df[\"text\"].tolist()\n",
    "train_labels = train_df[\"label\"].tolist()\n",
    "dev_texts = dev_df[\"text\"].tolist()\n",
    "dev_labels = dev_df[\"label\"].tolist()\n",
    "test_texts = test_df[\"text\"].tolist()\n",
    "test_labels = test_df[\"label\"].tolist()\n",
    "\n",
    "train_dataset = MixedTextDataset(train_texts, train_labels, tokenizer, max_len=config['max_len'])\n",
    "dev_dataset = MixedTextDataset(dev_texts, dev_labels, tokenizer, max_len=config['max_len'])\n",
    "test_dataset = MixedTextDataset(test_texts, test_labels, tokenizer, max_len=config['max_len'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_labels = 2\n",
    "\n",
    "model = DistilBERTBiGRUCRFTagger(MODEL_NAME, num_labels).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "total_steps = len(train_loader) * config['epochs']\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=config['learning_rate'],\n",
    "                                               total_steps=total_steps, pct_start=0.1, anneal_strategy='cos')\n",
    "\n",
    "best_kappa = -float('inf')\n",
    "best_epoch = 0\n",
    "best_model_state = None\n",
    "train_losses = []\n",
    "val_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'mcc': [], 'mae': [], 'std_dev': [], 'kappa': []}\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
    "    train_loss = train_model(model, train_loader, optimizer, scheduler, device, config['gradient_clip'])\n",
    "    train_losses.append(train_loss)\n",
    "    val_accuracy, val_precision, val_recall, val_f1, val_mcc, val_mae, val_std_dev, val_kappa = evaluate_model(model, dev_loader, device)\n",
    "    val_metrics['accuracy'].append(val_accuracy)\n",
    "    val_metrics['precision'].append(val_precision)\n",
    "    val_metrics['recall'].append(val_recall)\n",
    "    val_metrics['f1'].append(val_f1)\n",
    "    val_metrics['mcc'].append(val_mcc)\n",
    "    val_metrics['mae'].append(val_mae)\n",
    "    val_metrics['std_dev'].append(val_std_dev)\n",
    "    val_metrics['kappa'].append(val_kappa)\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Validation Metrics:\")\n",
    "    print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "    print(f\"Precision: {val_precision:.4f}\")\n",
    "    print(f\"Recall: {val_recall:.4f}\")\n",
    "    print(f\"F1 Score: {val_f1:.4f}\")\n",
    "    print(f\"MCC: {val_mcc:.4f}\")\n",
    "    print(f\"MAE: {val_mae:.2f}±{val_std_dev:.2f}\")\n",
    "    print(f\"Kappa: {val_kappa:.4f}\")\n",
    "    if val_kappa > best_kappa:\n",
    "        best_kappa = val_kappa\n",
    "        best_epoch = epoch + 1\n",
    "        best_model_state = model.state_dict()\n",
    "        print(f\"New best model with Kappa Score: {val_kappa:.4f}\")\n",
    "\n",
    "print(f\"\\nBest model at epoch {best_epoch} with Kappa Score: {best_kappa:.4f}\")\n",
    "\n",
    "model.load_state_dict(best_model_state)\n",
    "model.eval()\n",
    "test_accuracy, test_precision, test_recall, test_f1, test_mcc, test_mae, test_std_dev, test_kappa = evaluate_model(model, test_loader, device)\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Precision: {test_precision:.4f}\")\n",
    "print(f\"Recall: {test_recall:.4f}\")\n",
    "print(f\"F1 Score: {test_f1:.4f}\")\n",
    "print(f\"MCC: {test_mcc:.4f}\")\n",
    "print(f\"MAE: {test_mae:.2f}±{test_std_dev:.2f}\")\n",
    "print(f\"Kappa: {test_kappa:.4f}\")\n",
    "\n",
    "test_df_copy = test_df.copy()\n",
    "test_df_copy[\"predicted_boundary\"] = test_df_copy[\"text\"].apply(\n",
    "    lambda x: predict_boundary(model, x, tokenizer, max_len=config['max_len'], device=device)\n",
    ")\n",
    "difference = (test_df_copy['label'] - test_df_copy['predicted_boundary']).abs()\n",
    "boundary_mae = difference.mean()\n",
    "boundary_sd = difference.std()\n",
    "print(f\"Boundary MAE ± SD: {boundary_mae:.4f} ± {boundary_sd:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3b41ac-0a4c-4ff5-9995-922cd51af81d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
